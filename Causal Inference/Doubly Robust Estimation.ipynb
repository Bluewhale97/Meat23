{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dd8925e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install your library\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import graphviz as gr\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5c2b5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/zjy97/Downloads/python-causality-handbook-v1.0/matheusfacure-python-causality-handbook-f666303/causal-inference-for-the-brave-and-true/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "54157032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10391, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3882218074580043"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. doubly robust estimation is combining propensity score and linear regression \n",
    "\n",
    "# let's run a case\n",
    "data = pd.read_csv(path+'learning_mindset.csv')\n",
    "\n",
    "# convert categorical variables into dummies\n",
    "categ = [\"ethnicity\", \"gender\", \"school_urbanicity\"]\n",
    "cont = [\"school_mindset\", \"school_achievement\", \"school_ethnic_minority\", \"school_poverty\", \"school_size\"]\n",
    "\n",
    "data_with_categ = pd.concat([\n",
    "    data.drop(columns=categ), # dataset without the categorical features\n",
    "    pd.get_dummies(data[categ], columns=categ, drop_first=False) # categorical features converted to dummies\n",
    "], axis=1)\n",
    "\n",
    "print(data_with_categ.shape)\n",
    "\n",
    "# make a doubly robust regression\n",
    "# to compute ATE of a robust regression ATE = 1/N (the sum of u1+ T(Y-mu1)/P(Xi)) - 1/N (the sum of u0 + (1-T)(Y-mu0)/(1-P(X)))\n",
    "\n",
    "\n",
    "def doubly_robust(df, X, T, Y):\n",
    "    ps = LogisticRegression(C=1e6, max_iter=1000).fit(df[X], df[T]).predict_proba(df[X])[:, 1]\n",
    "    mu0 = LinearRegression().fit(df.query(f\"{T}==0\")[X], df.query(f\"{T}==0\")[Y]).predict(df[X])\n",
    "    mu1 = LinearRegression().fit(df.query(f\"{T}==1\")[X], df.query(f\"{T}==1\")[Y]).predict(df[X])\n",
    "    return (\n",
    "        np.mean(df[T]*(df[Y] - mu1)/ps + mu1) -\n",
    "        np.mean((1-df[T])*(df[Y] - mu0)/(1-ps) + mu0)\n",
    "    )\n",
    "\n",
    "T = 'intervention'\n",
    "Y = 'achievement_score'\n",
    "X = data_with_categ.columns.drop(['schoolid', T, Y])\n",
    "\n",
    "doubly_robust(data_with_categ, X, T, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1804fdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doubly robust estimator is saying that we have 0.388 std as ATE\n",
    "\n",
    "# let's boostrap to construct confidence intervals\n",
    "from joblib import Parallel, delayed # for parallel processing\n",
    "\n",
    "np.random.seed(88)\n",
    "# run 1000 bootstrap samples\n",
    "bootstrap_sample = 1000\n",
    "ates = Parallel(n_jobs=4)(delayed(doubly_robust)(data_with_categ.sample(frac=1, replace=True), X, T, Y)\n",
    "                          for _ in range(bootstrap_sample))\n",
    "ates = np.array(ates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056eeaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ATE 95% CI:\", (np.percentile(ates, 2.5), np.percentile(ates, 97.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f6ca047b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ATE 95% CI: (0.3536550125013336, 0.41978398695632074)\n",
      "Wrong PS ATE 95% CI: (0.33853073645584153, 0.4328333217479126)\n"
     ]
    }
   ],
   "source": [
    "# 2. test the doubly robust estimater when ps doesnt work\n",
    "\n",
    "# let's test when the propensity score model is done, whether we can still get a good result from regression\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "def doubly_robust_wrong_ps(df, X, T, Y):\n",
    "    # wrong PS model\n",
    "    np.random.seed(654)\n",
    "    ps = np.random.uniform(0.1, 0.9, df.shape[0])\n",
    "    mu0 = LinearRegression().fit(df.query(f\"{T}==0\")[X], df.query(f\"{T}==0\")[Y]).predict(df[X])\n",
    "    mu1 = LinearRegression().fit(df.query(f\"{T}==1\")[X], df.query(f\"{T}==1\")[Y]).predict(df[X])\n",
    "    return (\n",
    "        np.mean(df[T]*(df[Y] - mu1)/ps + mu1) -\n",
    "        np.mean((1-df[T])*(df[Y] - mu0)/(1-ps) + mu0)\n",
    "    )\n",
    "\n",
    "doubly_robust_wrong_ps(data_with_categ, X, T, Y)# 0.379, almost same!\n",
    "\n",
    "# also do a bootstrapping\n",
    "np.random.seed(88)\n",
    "parallel_fn = delayed(doubly_robust_wrong_ps)\n",
    "wrong_ps = Parallel(n_jobs=4)(parallel_fn(data_with_categ.sample(frac=1, replace=True), X, T, Y)\n",
    "                              for _ in range(bootstrap_sample))\n",
    "wrong_ps = np.array(wrong_ps)\n",
    "\n",
    "print(f\"Original ATE 95% CI:\", (np.percentile(ates, 2.5), np.percentile(ates, 97.5)))\n",
    "\n",
    "print(f\"Wrong PS ATE 95% CI:\", (np.percentile(wrong_ps, 2.5), np.percentile(wrong_ps, 97.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bf05c23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ATE 95% CI: (0.3536550125013336, 0.41978398695632074)\n",
      "Wrong Mu ATE 95% CI: (0.33871399647293654, 0.43316495544110667)\n"
     ]
    }
   ],
   "source": [
    "# 3. test if we have wrong regression how ps model work\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "\n",
    "def doubly_robust_wrong_model(df, X, T, Y):\n",
    "    np.random.seed(654)\n",
    "    ps = LogisticRegression(C=1e6, max_iter=1000).fit(df[X], df[T]).predict_proba(df[X])[:, 1]\n",
    "    \n",
    "    # wrong mu(x) model\n",
    "    mu0 = np.random.normal(0, 1, df.shape[0])\n",
    "    mu1 = np.random.normal(0, 1, df.shape[0])\n",
    "    return (\n",
    "        np.mean(df[T]*(df[Y] - mu1)/ps + mu1) -\n",
    "        np.mean((1-df[T])*(df[Y] - mu0)/(1-ps) + mu0)\n",
    "    )\n",
    "\n",
    "doubly_robust_wrong_model(data_with_categ, X, T, Y)\n",
    "\n",
    "# use bootstrapping as well\n",
    "np.random.seed(88)\n",
    "parallel_fn = delayed(doubly_robust_wrong_model)\n",
    "wrong_mux = Parallel(n_jobs=4)(parallel_fn(data_with_categ.sample(frac=1, replace=True), X, T, Y)\n",
    "                               for _ in range(bootstrap_sample))\n",
    "wrong_mux = np.array(wrong_mux)\n",
    "\n",
    "print(f\"Original ATE 95% CI:\", (np.percentile(ates, 2.5), np.percentile(ates, 97.5)))\n",
    "print(f\"Wrong Mu ATE 95% CI:\", (np.percentile(wrong_mux, 2.5), np.percentile(wrong_mux, 97.5)))\n",
    "\n",
    "\n",
    "# 4. a findout\n",
    "# doubly robust estimate has narrower CI when doing boostrapping, then only one of the methods like propensity score or regression called\n",
    "# this is because ut can provide a more efficient estimate of the treatment effect when two are all work\n",
    "# but there is still some biases "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
