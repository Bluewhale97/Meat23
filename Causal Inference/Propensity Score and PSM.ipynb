{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "77bc9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install your library\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import graphviz as gr\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f1e6f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/zjy97/Downloads/python-causality-handbook-v1.0/matheusfacure-python-causality-handbook-f666303/causal-inference-for-the-brave-and-true/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e1545a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10391, 32)\n"
     ]
    }
   ],
   "source": [
    "# 1. what is propensity score\n",
    "# you dont need to directly control for confounders X to achieve conditional indepence Y1,Y0 with T|X\n",
    "# it is sufficient to control for a balancing score E[T|X], and this is often the conditional probability of the treatment, P(T|X)\n",
    "\n",
    "# then we have (Y1,Y0) independent with T|e(x)\n",
    "\n",
    "# 2. let's see how propensity score estimation works\n",
    "data = pd.read_csv(path+\"learning_mindset.csv\")\n",
    "\n",
    "categ = [\"ethnicity\", \"gender\", \"school_urbanicity\"]\n",
    "cont = [\"school_mindset\", \"school_achievement\", \"school_ethnic_minority\", \"school_poverty\", \"school_size\"]\n",
    "\n",
    "data_with_categ = pd.concat([\n",
    "    data.drop(columns=categ), # dataset without the categorical features\n",
    "    pd.get_dummies(data[categ], columns=categ, drop_first=False)# categorical features converted to dummies\n",
    "], axis=1)\n",
    "\n",
    "print(data_with_categ.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b5c3929b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intervention</th>\n",
       "      <th>achievement_score</th>\n",
       "      <th>propensity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.277359</td>\n",
       "      <td>0.315487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.449646</td>\n",
       "      <td>0.263800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.769703</td>\n",
       "      <td>0.344024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.121763</td>\n",
       "      <td>0.344024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.526147</td>\n",
       "      <td>0.367784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   intervention  achievement_score  propensity_score\n",
       "0             1           0.277359          0.315487\n",
       "1             1          -0.449646          0.263800\n",
       "2             1           0.769703          0.344024\n",
       "3             1          -0.121763          0.344024\n",
       "4             1           1.526147          0.367784"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimate propensity score using logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "T = 'intervention'\n",
    "Y = 'achievement_score'\n",
    "X = data_with_categ.columns.drop(['schoolid', T, Y])\n",
    "\n",
    "ps_model = LogisticRegression(C=1e6).fit(data_with_categ[X], data_with_categ[T])\n",
    "\n",
    "data_ps = data.assign(propensity_score=ps_model.predict_proba(data_with_categ[X])[:, 1])\n",
    "\n",
    "data_ps[[\"intervention\", \"achievement_score\", \"propensity_score\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6c10c2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sample Size 10391\n",
      "Treated Population Sample Size 10388.57453106489\n",
      "Untreated Population Sample Size 10391.439059342569\n"
     ]
    }
   ],
   "source": [
    "# then, weigh each propensity score by types of treatment\n",
    "weight_t = 1/data_ps.query(\"intervention==1\")[\"propensity_score\"]\n",
    "weight_nt = 1/(1-data_ps.query(\"intervention==0\")[\"propensity_score\"])\n",
    "print(\"Original Sample Size\", data.shape[0])\n",
    "print(\"Treated Population Sample Size\", sum(weight_t))\n",
    "print(\"Untreated Population Sample Size\", sum(weight_nt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b60ae915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y1: 0.2595808315593797\n",
      "Y0: -0.1289233440296743\n",
      "ATE 0.38850417558905403\n"
     ]
    }
   ],
   "source": [
    "# another weight to interact with different treatment\n",
    "weight = ((data_ps[\"intervention\"]-data_ps[\"propensity_score\"]) /\n",
    "          (data_ps[\"propensity_score\"]*(1-data_ps[\"propensity_score\"])))\n",
    "\n",
    "y1 = sum(data_ps.query(\"intervention==1\")[\"achievement_score\"]*weight_t) / len(data)\n",
    "y0 = sum(data_ps.query(\"intervention==0\")[\"achievement_score\"]*weight_nt) / len(data)\n",
    "\n",
    "ate = np.mean(weight * data_ps[\"achievement_score\"])\n",
    "\n",
    "print(\"Y1:\", y1)\n",
    "print(\"Y0:\", y0)\n",
    "print(\"ATE\", ate)\n",
    "\n",
    "# this method is called inverse probability of treatmenty weighting\n",
    "# propensity score weighting is saying that we should expect treated individuals to be .38 sd above their untreated fellows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "9c42b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. standard error of ATE using propensity score with IPTW\n",
    "from joblib import Parallel, delayed # for parallel processing\n",
    "\n",
    "# define function that computes the IPTW estimator\n",
    "def run_ps(df, X, T, y):\n",
    "    # estimate the propensity score\n",
    "    ps = LogisticRegression(C=1e6, max_iter=1000).fit(df[X], df[T]).predict_proba(df[X])[:, 1]\n",
    "    \n",
    "    weight = (df[T]-ps) / (ps*(1-ps)) # define the weights\n",
    "    return np.mean(weight * df[y]) # compute the ATE\n",
    "\n",
    "np.random.seed(88)\n",
    "# run 1000 bootstrap samples\n",
    "bootstrap_sample = 1000\n",
    "ates = Parallel(n_jobs=4)(delayed(run_ps)(data_with_categ.sample(frac=1, replace=True), X, T, Y)\n",
    "                          for _ in range(bootstrap_sample))\n",
    "ates = np.array(ates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "bc27f191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE: 0.38774590065633185\n",
      "95% C.I.: (0.3545141663026032, 0.41992559665045304)\n"
     ]
    }
   ],
   "source": [
    "print(f\"ATE: {ates.mean()}\")\n",
    "print(f\"95% C.I.: {(np.percentile(ates, 2.5), np.percentile(ates, 97.5))}\")# ate is 0.38 and 95% CI is (0.3545132414290843, 0.41992560836402076)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b869b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. common issues with propensity score\n",
    "# actually the predictive quality of the propensity score does not translate into its balancing properties\n",
    "# what we want to do on propensity score is to produce it as a intermiate variable and get rid of confounders, not prediction\n",
    "# even though we use fancy ml algo it can get good prediction\n",
    "# propensity score doesnt have to predict the treatment very well, it just needs to include all the confounding variables\n",
    "# if we incldue variables that are very good in predicting the treatment but have no bearing on the outcome it will actually increase the variance of the propensity score estimator\n",
    "# this is similar to the problem linear regression faces when we include variables correlated with the treatment but not with the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "edb4d0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treatment Effect Estimates: Matching\n",
      "\n",
      "                     Est.       S.e.          z      P>|z|      [95% Conf. int.]\n",
      "--------------------------------------------------------------------------------\n",
      "           ATE      0.389      0.025     15.595      0.000      0.340      0.438\n",
      "           ATC      0.380      0.027     13.907      0.000      0.327      0.434\n",
      "           ATT      0.406      0.027     15.226      0.000      0.354      0.459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. propensity score matching\n",
    "# we can think of propensity score as performing dimensionality reduction on the feature space\n",
    "# so previously we use inverse propensity score as weight times the outcome to get the ATE, it is good but the question is\n",
    "# propensity score can be a combination of confounders, but we havd SE on IPTW, then we have to compute CI and mean.\n",
    "# why do you not directly control the ps, then run with the treatment to predict outcome\n",
    "cm = CausalModel(\n",
    "    Y=data_ps[\"achievement_score\"].values, \n",
    "    D=data_ps[\"intervention\"].values, \n",
    "    X=data_ps[[\"propensity_score\"]].values\n",
    ")\n",
    "\n",
    "cm.est_via_matching(matches=1, bias_adj=True)\n",
    "\n",
    "print(cm.estimates)\n",
    "# so right now we have ATE 0.389, it is about same with the pstw\n",
    "# actually we also can drive standard deviation for ATE but bootstrap doesnt work with matching, there is no py lib for correct std\n",
    "\n",
    "# the reason is about inconsistency(bootstrapping cannot correclt poroducing the distribution of ATE), randomness in results(if treatemnet group is small, matching results in difference and randomness is huge) and estimation erros(cannot decide for how close for unit with score) \n",
    "# some researchers say that we could use Monte Carlo simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fde969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
